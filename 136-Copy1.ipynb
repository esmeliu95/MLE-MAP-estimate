{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"training_data.txt\", \"r\")\n",
    "train_N = text_file.read().split(' ') #N = Total no, of words from training set\n",
    "K = list(set(train_N)) #K = 10,000 distinct words\n",
    "c = collections.Counter(train_N)#c is a dictionary of 10,000 distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = open(\"test_data.txt\", \"r\")\n",
    "testing = new_test.read().split(' ') #all the words from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = []\n",
    "training_sets.append(np.random.permutation(train_N)[:5000]) #N/128\n",
    "training_sets.append(np.random.permutation(train_N)[:10000]) #N/64 \n",
    "training_sets.append(np.random.permutation(train_N)[:40000]) #N/16\n",
    "training_sets.append(np.random.permutation(train_N)[:160000]) #N/4\n",
    "training_sets.append(train_N) #N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "models = []\n",
    "for each_train in training_sets:\n",
    "    dictionary = collections.Counter(each_train)\n",
    "    N = len(each_train) #5000 10000 ... 640000\n",
    "    each_train = list(set(each_train)) #get rid of dupes\n",
    "    model = collections.Counter()\n",
    "    for word in each_train:\n",
    "        model[word] = dictionary[word]/N\n",
    "        \n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, run_test):\n",
    "    \n",
    "    N = len(run_test)\n",
    "    run_test_no_dupe = list(set(run_test))\n",
    "    probability = np.zeros(len(run_test_no_dupe))\n",
    "    for index,word in enumerate(run_test_no_dupe):\n",
    "        probability[index] = model[word]\n",
    "\n",
    "    temp = probability.copy()\n",
    "    temp[temp!=0] = np.log(temp[temp!=0])\n",
    "    \n",
    "    pp = np.exp((-1/N)*np.sum(temp))\n",
    "    print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,model in enumerate(models):\n",
    "\n",
    "    print('train perplexity: ')\n",
    "    perplexity(model,training_sets[index])\n",
    "\n",
    "    print('test perplxity:')\n",
    "    perplexity(model,testing)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perplexity\n",
    "def perplexity_training():\n",
    "    for each_train in training_sets:\n",
    "        dictionary = collections.Counter(each_train)\n",
    "        N = len(each_train)\n",
    "        each_train = list(set(each_train))\n",
    "        probability = np.zeros(len(each_train))\n",
    "        probability = [(dictionary[i]/N) for i in each_train]\n",
    "        temp = probability.copy()\n",
    "        temp = np.log(temp)\n",
    "        pp = np.exp((-1/N)*np.sum(temp))\n",
    "        print('pp ',pp,' N size is ', N,' counter size is ',len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
